from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
import time

# ============================================================
# 1ï¸âƒ£ Khá»Ÿi táº¡o SparkSession vá»›i Delta Lake support
# ============================================================
spark = (
    SparkSession.builder
        .appName("Delta Lake Streaming Test")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.sql.streaming.checkpointLocation", "s3a://bronze/checkpoints")
        .enableHiveSupport()
        .getOrCreate()
)

# Set log level to reduce noise
spark.sparkContext.setLogLevel("WARN")

# ============================================================
# 2ï¸âƒ£ Define schema for streaming data
# ============================================================
schema = StructType([
    StructField("id", IntegerType(), False),
    StructField("name", StringType(), True),
    StructField("value", IntegerType(), True),
    StructField("timestamp", TimestampType(), True)
])

# ============================================================
# 3ï¸âƒ£ Create streaming DataFrame using rate source (generates data)
# ============================================================
print("ğŸš€ Starting streaming job...")
print("ğŸ“Š Generating streaming data using rate source...")

# Rate source generates data at specified rate (rows per second)
streaming_df = (
    spark.readStream
        .format("rate")
        .option("rowsPerSecond", 5)  # Generate 5 rows per second
        .load()
        .withColumn("id", F.col("value").cast("integer"))
        .withColumn("name", F.concat(F.lit("User_"), F.col("value")))
        .withColumn("event_type", 
                   F.when(F.col("value") % 3 == 0, "purchase")
                    .when(F.col("value") % 3 == 1, "view")
                    .otherwise("click"))
        .withColumn("amount", (F.rand() * 1000).cast("integer"))
        .withColumn("ingest_ts", F.col("timestamp"))
        .select("id", "name", "event_type", "amount", "ingest_ts")
)

# ============================================================
# 4ï¸âƒ£ Define Delta Lake table location
# ============================================================
db_name = "bronze"
table_name = "streaming_events"
full_table_name = f"{db_name}.{table_name}"
delta_path = f"s3a://{db_name}/{table_name}"
checkpoint_path = f"s3a://{db_name}/checkpoints/{table_name}"

# ============================================================
# 5ï¸âƒ£ Create database if not exists
# ============================================================
print(f"ğŸ“‹ Creating database: {db_name}")
spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")

# ============================================================
# 6ï¸âƒ£ Write streaming data to Delta Lake
# ============================================================
print(f"ğŸ“ Writing streaming data to Delta table: {delta_path}")
print(f"ğŸ“ Checkpoint location: {checkpoint_path}")
print(f"â±ï¸  Trigger interval: 10 seconds")
print("=" * 60)

query = (
    streaming_df.writeStream
        .format("delta")
        .outputMode("append")  # For streaming, use append mode
        .option("checkpointLocation", checkpoint_path)
        .trigger(processingTime="10 seconds")  # Micro-batch every 10 seconds
        .option("path", delta_path)
        .start()
)

# ============================================================
# 7ï¸âƒ£ Monitor streaming query
# ============================================================
print("âœ… Streaming query started!")
print("ğŸ“Š Query ID:", query.id)
print("ğŸ“Œ Query Name:", query.name if query.name else "Unnamed")
print("=" * 60)

# Run for 60 seconds to collect some data
duration = 60
print(f"â³ Running streaming for {duration} seconds...")
print("ğŸ’¡ You can stop early with Ctrl+C")
print("=" * 60)

try:
    for i in range(duration):
        time.sleep(1)
        if i % 10 == 0 and i > 0:
            status = query.status
            print(f"\nğŸ“ˆ Progress after {i} seconds:")
            print(f"   - Is Active: {query.isActive}")
            print(f"   - Message: {status['message']}")
            if 'numInputRows' in status:
                print(f"   - Input Rows: {status['numInputRows']}")
            
    print("\nâ¹ï¸  Stopping streaming query...")
    query.stop()
    query.awaitTermination(timeout=10)
    
except KeyboardInterrupt:
    print("\nâš ï¸  Interrupted by user. Stopping query...")
    query.stop()
    query.awaitTermination(timeout=10)

# ============================================================
# 8ï¸âƒ£ Register table in Hive Metastore
# ============================================================
print(f"\nğŸ“‹ Registering table in Hive Metastore: {full_table_name}")

try:
    spark.sql(f"DROP TABLE IF EXISTS {full_table_name}")
    
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {full_table_name}
    USING DELTA
    LOCATION '{delta_path}'
    """
    spark.sql(create_table_sql)
    print(f"âœ… Table registered: {full_table_name}")
except Exception as e:
    print(f"âŒ Error during table registration: {e}")

# ============================================================
# 9ï¸âƒ£ Verify data was written
# ============================================================
print("\nğŸ” Verifying written data...")
try:
    result = spark.sql(f"SELECT * FROM {full_table_name} ORDER BY id DESC LIMIT 10")
    print(f"\nğŸ“Š Latest 10 records in {full_table_name}:")
    result.show(truncate=False)
    
    count_df = spark.sql(f"SELECT COUNT(*) as total_records FROM {full_table_name}")
    total = count_df.collect()[0]['total_records']
    print(f"\nâœ… SUCCESS! Total records in table: {total}")
    
    # Show event type distribution
    print(f"\nğŸ“ˆ Event type distribution:")
    spark.sql(f"""
        SELECT event_type, COUNT(*) as count 
        FROM {full_table_name} 
        GROUP BY event_type
        ORDER BY count DESC
    """).show()
    
except Exception as e:
    print(f"âŒ Table query failed: {e}")

print("\n" + "=" * 60)
print("âœ… STREAMING TEST COMPLETE!")
print(f"ğŸ“‚ Delta table location: {delta_path}")
print(f"ğŸ“ Checkpoint location: {checkpoint_path}")
print(f"ğŸ—ƒï¸  Hive table: {full_table_name}")
print("ğŸ’¡ You can query this table from Trino or Spark SQL")
print("=" * 60)

spark.stop()
