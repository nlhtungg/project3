from pyspark.sql import SparkSession
from pyspark.sql import functions as F
import time

# ============================================================
# 1ï¸âƒ£ Khá»Ÿi táº¡o SparkSession vá»›i Delta Lake support
# ============================================================
spark = (
    SparkSession.builder
        .appName("Delta Lake Read Stream Test")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.sql.streaming.checkpointLocation", "s3a://bronze/checkpoints")
        .config("spark.driver.memory", "2g")
        .config("spark.executor.memory", "2g")
        .config("spark.driver.cores", "2")
        .config("spark.executor.cores", "2")
        .config("spark.executor.instances", "1")
        .config("spark.cores.max", "2")
        .enableHiveSupport()
        .getOrCreate()
)

# Set log level to reduce noise
spark.sparkContext.setLogLevel("WARN")

# ============================================================
# 2ï¸âƒ£ Define source and target paths
# ============================================================
source_db = "bronze"
source_table = "streaming_events"
source_delta_path = f"s3a://{source_db}/{source_table}"

target_db = "bronze"
target_table = "streaming_events_processed"
target_delta_path = f"s3a://{target_db}/{target_table}"
checkpoint_path = f"s3a://{target_db}/checkpoints/{target_table}"

print("=" * 60)
print("ğŸš€ Starting Delta Lake Streaming Read Test")
print("=" * 60)
print(f"ğŸ“– Source: {source_delta_path}")
print(f"ğŸ“ Target: {target_delta_path}")
print(f"ğŸ“ Checkpoint: {checkpoint_path}")
print("=" * 60)

# ============================================================
# 3ï¸âƒ£ Read streaming data from Delta Lake
# ============================================================
print(f"\nğŸ“– Reading stream from Delta table: {source_delta_path}")

streaming_read_df = (
    spark.readStream
        .format("delta")
        .option("ignoreChanges", "true")  # Important for reading from Delta
        .option("ignoreDeletes", "true")  # Ignore delete operations
        .load(source_delta_path)
)

print("âœ… Stream reading configured")
print(f"ğŸ“Š Schema:")
streaming_read_df.printSchema()

# ============================================================
# 4ï¸âƒ£ Apply transformations on streaming data
# ============================================================
print("\nğŸ”„ Applying transformations...")

# Add aggregations and transformations
processed_df = (
    streaming_read_df
        # Add processing timestamp
        .withColumn("processing_ts", F.current_timestamp())
        
        # Calculate revenue tier based on amount
        .withColumn("revenue_tier", 
                   F.when(F.col("amount") < 200, "low")
                    .when((F.col("amount") >= 200) & (F.col("amount") < 600), "medium")
                    .when((F.col("amount") >= 600) & (F.col("amount") < 900), "high")
                    .otherwise("premium"))
        
        # Add day of week
        .withColumn("day_of_week", F.dayofweek(F.col("ingest_ts")))
        
        # Add hour of day
        .withColumn("hour_of_day", F.hour(F.col("ingest_ts")))
        
        # Calculate time difference
        .withColumn("processing_delay_seconds", 
                   F.unix_timestamp(F.col("processing_ts")) - 
                   F.unix_timestamp(F.col("ingest_ts")))
)

print("âœ… Transformations configured:")
print("   - Added processing_ts")
print("   - Added revenue_tier categorization")
print("   - Added day_of_week and hour_of_day")
print("   - Calculated processing_delay_seconds")

# ============================================================
# 5ï¸âƒ£ Create database if not exists
# ============================================================
print(f"\nğŸ“‹ Creating database: {target_db}")
spark.sql(f"CREATE DATABASE IF NOT EXISTS {target_db}")

# ============================================================
# 6ï¸âƒ£ Write processed stream to another Delta table
# ============================================================
print(f"\nğŸ“ Writing processed stream to: {target_delta_path}")
print(f"â±ï¸  Trigger interval: 15 seconds")
print("=" * 60)

query = (
    processed_df.writeStream
        .format("delta")
        .outputMode("append")
        .option("checkpointLocation", checkpoint_path)
        .trigger(processingTime="15 seconds")
        .option("path", target_delta_path)
        .start()
)

# ============================================================
# 7ï¸âƒ£ Also write to console for monitoring
# ============================================================
print("\nğŸ“º Starting console output stream (for monitoring)...")

console_query = (
    processed_df
        .select("id", "name", "event_type", "amount", "revenue_tier", "processing_delay_seconds")
        .writeStream
        .format("console")
        .outputMode("append")
        .trigger(processingTime="15 seconds")
        .option("truncate", "false")
        .start()
)

# ============================================================
# 8ï¸âƒ£ Monitor streaming queries
# ============================================================
print("âœ… Streaming queries started!")
print("\nğŸ“Š Main Query:")
print(f"   - Query ID: {query.id}")
print(f"   - Is Active: {query.isActive}")
print("\nğŸ“º Console Query:")
print(f"   - Query ID: {console_query.id}")
print(f"   - Is Active: {console_query.isActive}")
print("=" * 60)

# Run for 90 seconds
duration = 90
print(f"\nâ³ Running streaming for {duration} seconds...")
print("ğŸ’¡ You can stop early with Ctrl+C")
print("=" * 60)

try:
    for i in range(duration):
        time.sleep(1)
        if i % 15 == 0 and i > 0:
            status = query.status
            console_status = console_query.status
            
            print(f"\nğŸ“ˆ Progress after {i} seconds:")
            print(f"   Main Query:")
            print(f"   - Is Active: {query.isActive}")
            print(f"   - Message: {status['message']}")
            
            print(f"   Console Query:")
            print(f"   - Is Active: {console_query.isActive}")
            print(f"   - Message: {console_status['message']}")
            print("=" * 60)
    
    print("\nâ¹ï¸  Stopping streaming queries...")
    query.stop()
    console_query.stop()
    query.awaitTermination(timeout=10)
    console_query.awaitTermination(timeout=10)
    
except KeyboardInterrupt:
    print("\nâš ï¸  Interrupted by user. Stopping queries...")
    query.stop()
    console_query.stop()
    query.awaitTermination(timeout=10)
    console_query.awaitTermination(timeout=10)

# ============================================================
# 9ï¸âƒ£ Register table in Hive Metastore
# ============================================================
full_target_table = f"{target_db}.{target_table}"
print(f"\nğŸ“‹ Registering table in Hive Metastore: {full_target_table}")

try:
    spark.sql(f"DROP TABLE IF EXISTS {full_target_table}")
    
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {full_target_table}
    USING DELTA
    LOCATION '{target_delta_path}'
    """
    spark.sql(create_table_sql)
    print(f"âœ… Table registered: {full_target_table}")
except Exception as e:
    print(f"âŒ Error during table registration: {e}")

# ============================================================
# ğŸ”Ÿ Verify processed data
# ============================================================
print("\nğŸ” Verifying processed data...")
try:
    result = spark.sql(f"SELECT * FROM {full_target_table} ORDER BY id DESC LIMIT 10")
    print(f"\nğŸ“Š Latest 10 processed records:")
    result.show(truncate=False)
    
    count_df = spark.sql(f"SELECT COUNT(*) as total_records FROM {full_target_table}")
    total = count_df.collect()[0]['total_records']
    print(f"\nâœ… Total processed records: {total}")
    
    # Show revenue tier distribution
    print(f"\nğŸ’° Revenue tier distribution:")
    spark.sql(f"""
        SELECT revenue_tier, COUNT(*) as count, AVG(amount) as avg_amount
        FROM {full_target_table} 
        GROUP BY revenue_tier
        ORDER BY avg_amount DESC
    """).show()
    
    # Show event type breakdown by revenue tier
    print(f"\nğŸ“ˆ Event type by revenue tier:")
    spark.sql(f"""
        SELECT event_type, revenue_tier, COUNT(*) as count
        FROM {full_target_table} 
        GROUP BY event_type, revenue_tier
        ORDER BY event_type, revenue_tier
    """).show()
    
    # Show average processing delay
    print(f"\nâ±ï¸  Processing delay statistics:")
    spark.sql(f"""
        SELECT 
            AVG(processing_delay_seconds) as avg_delay,
            MIN(processing_delay_seconds) as min_delay,
            MAX(processing_delay_seconds) as max_delay
        FROM {full_target_table}
    """).show()
    
except Exception as e:
    print(f"âŒ Table query failed: {e}")

print("\n" + "=" * 60)
print("âœ… STREAMING READ TEST COMPLETE!")
print("=" * 60)
print(f"ğŸ“– Source table: {source_db}.{source_table}")
print(f"ğŸ“ Target table: {full_target_table}")
print(f"ğŸ“‚ Delta location: {target_delta_path}")
print(f"ğŸ“ Checkpoint: {checkpoint_path}")
print("\nğŸ’¡ Key Features Demonstrated:")
print("   âœ… Read streaming data from Delta Lake")
print("   âœ… Apply transformations on streaming data")
print("   âœ… Write to another Delta table")
print("   âœ… Console output for monitoring")
print("   âœ… Revenue tier categorization")
print("   âœ… Processing delay calculation")
print("=" * 60)

spark.stop()
