from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from delta.tables import DeltaTable

# ============================================================
# 1ï¸âƒ£ Khá»Ÿi táº¡o SparkSession (Ä‘Ã£ cÃ³ config tá»« spark-defaults.conf & hive-site.xml)
# ============================================================
spark = (
    SparkSession.builder
        .appName("Write Bronze Delta Table")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .enableHiveSupport()  # cáº§n cho saveAsTable -> Hive Metastore
        .getOrCreate()
)

# ============================================================
# 2ï¸âƒ£ Táº¡o DataFrame máº«u
# ============================================================
data = [
    (1, "Alice", "2025-10-28"),
    (2, "Bob", "2025-10-28"),
    (3, "Charlie Puth", "2025-10-28"),
]
columns = ["id", "name", "ingest_date"]

df = spark.createDataFrame(data, columns)
df = df.withColumn("ingest_ts", F.current_timestamp())

# ============================================================
# 3ï¸âƒ£ Khai bÃ¡o database, table, path
# ============================================================
db_name = "bronze"
table_name = "peoples"
full_table_name = f"{db_name}.{table_name}"
delta_path = f"s3a://{db_name}/{table_name}"

# ============================================================
# 4ï¸âƒ£ Táº¡o database náº¿u chÆ°a cÃ³ (Ä‘Äƒng kÃ½ trong Hive)
# ============================================================
print(f"ğŸ“‹ Creating database: {db_name}")
spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")
print(f"âœ… Database created/verified")

# ============================================================
# 5ï¸âƒ£ Ghi dá»¯ liá»‡u Delta báº±ng UPSERT (MERGE)
# ============================================================

print(f"ğŸ“ Upserting data to Delta table: {delta_path}")

# Check if table exists
try:
    # Table exists - perform MERGE (upsert)
    deltaTable = DeltaTable.forPath(spark, delta_path)
    
    print("ğŸ”„ Table exists, performing MERGE (upsert)...")
    (
        deltaTable.alias("target")
        .merge(
            df.alias("source"),
            "target.id = source.id"  # Merge condition (primary key)
        )
        .whenMatchedUpdateAll()  # Update existing records
        .whenNotMatchedInsertAll()  # Insert new records
        .execute()
    )
    print(f"âœ… Data upserted successfully!")
    
except Exception as e:
    # Table doesn't exist - create it with initial data
    print(f"ğŸ“ Table doesn't exist, creating new table...")
    (
        df.write
          .format("delta")
          .mode("overwrite")
          .option("overwriteSchema", "true")
          .save(delta_path)
    )
    print(f"âœ… Initial data written successfully!")

print(f"ğŸ“‚ Location: {delta_path}")

# ============================================================
# 6ï¸âƒ£ ÄÄƒng kÃ½ báº£ng trong Hive Metastore báº±ng CREATE TABLE
# ============================================================
print(f"\nğŸ“‹ Registering table in Hive Metastore: {full_table_name}")

# Create external table pointing to Delta location
try:
    spark.sql(f"DROP TABLE IF EXISTS {full_table_name}")
    print(f"ğŸ—‘ï¸ Dropped existing table (if any)")
    
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {full_table_name}
    USING DELTA
    LOCATION '{delta_path}'
    """
    spark.sql(create_table_sql)
    print(f"âœ… Table registered: {full_table_name}")
except Exception as e:
    print(f"âŒ Error during table registration: {e}")
    raise

# ============================================================
# 7ï¸âƒ£ Kiá»ƒm tra láº¡i Ä‘Äƒng kÃ½ Hive
# ============================================================
print("\nğŸ” Verifying table...")
try:
    spark.sql(f"SHOW TABLES IN {db_name}").show(truncate=False)
    print("\nğŸ“Š Querying table via Spark SQL...")
    result = spark.sql(f"SELECT * FROM {full_table_name}")
    result.show(truncate=False)
    print(f"\nâœ… SUCCESS! Row count: {result.count()}")
except Exception as e:
    print(f"âŒ Table query failed: {e}")
    raise

print("\nâœ… DONE! Table is registered in Hive and can be queried via Trino!")

spark.stop()
