# ================================================
# Spark - Multi-Job Resource Management Configuration
# Optimized for concurrent Bronze/Silver streaming jobs
# ================================================

spark.master=spark://spark-master:7077
spark.sql.catalogImplementation=hive
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Hive Metastore URI (trỏ đến container Hive)
spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083
spark.hadoop.hive.metastore.warehouse.dir=s3a://bronze/

# -----------------------------------------------
# Multi-Job Resource Management
# -----------------------------------------------
# Enable dynamic allocation for better resource sharing
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.schedulerBacklogTimeout=5s
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=10s
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.cachedExecutorIdleTimeout=120s

# Global resource limits to prevent any single job from consuming all resources
spark.executor.memory=2g
spark.driver.memory=2g
spark.executor.cores=1
spark.cores.max=8
spark.sql.shuffle.partitions=20

# Memory management settings to prevent allocation failures
spark.executor.memoryOverhead=512m
spark.driver.memoryOverhead=512m
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5

# CRITICAL: Disable broadcast joins to prevent OOM errors
spark.sql.autoBroadcastJoinThreshold=-1

# Shuffle stability settings
spark.shuffle.service.enabled=false
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.io.compression.codec=snappy
spark.reducer.maxSizeInFlight=24m
spark.shuffle.file.buffer=16k

# Fair scheduler for resource sharing between multiple applications
spark.scheduler.mode=FAIR

# -----------------------------------------------
# Streaming Optimizations
# -----------------------------------------------
# Enable backpressure to prevent overwhelming the system
spark.streaming.backpressure.enabled=true
spark.streaming.backpressure.initialRate=1000
spark.streaming.receiver.maxRate=2000
spark.streaming.kafka.maxRatePerPartition=1000

# Graceful shutdown for streaming jobs
spark.streaming.stopGracefullyOnShutdown=true
spark.streaming.gracefulStopTimeout=30s

# -----------------------------------------------
# S3 / MinIO
# -----------------------------------------------
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore

# S3A optimizations for concurrent access
spark.hadoop.fs.s3a.connection.maximum=50
spark.hadoop.fs.s3a.threads.max=20
spark.hadoop.fs.s3a.max.total.tasks=20
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.multipart.size=52428800

# -----------------------------------------------
# Performance Optimizations for Multiple Jobs
# -----------------------------------------------
# Adaptive Query Execution
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionSize=32MB
spark.sql.adaptive.advisoryPartitionSizeInBytes=64MB
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true

# Memory management
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe=true
spark.sql.execution.arrow.pyspark.enabled=true

# Delta Lake optimizations for concurrent access
spark.databricks.delta.optimizeWrite.enabled=true
spark.databricks.delta.autoCompact.enabled=true
spark.databricks.delta.autoCompact.minNumFiles=10
spark.databricks.delta.merge.enableLowShuffle=true

# -----------------------------------------------
# Prometheus Metrics
# -----------------------------------------------
spark.ui.prometheus.enabled=true
spark.metrics.conf.*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
spark.metrics.conf.*.sink.prometheusServlet.path=/metrics/prometheus
spark.metrics.conf.master.sink.prometheusServlet.path=/metrics/master/prometheus
spark.metrics.conf.applications.sink.prometheusServlet.path=/metrics/applications/prometheus

# -----------------------------------------------
# Logging / Misc
# -----------------------------------------------
spark.eventLog.enabled=false
spark.sql.warehouse.dir=s3a://bronze/
spark.sql.adaptive.logLevel=WARN
